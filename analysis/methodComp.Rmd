---
title: "Iterative Comparison"
output:
  workflowr::wflow_html:
    toc: true
    latex_engine: "xelatex"
    code_folding: "hide"
editor_options:
  chunk_output_type: console
---
  
```{r 0-setup, include=FALSE, warning=FALSE}
#regular
library(dplyr)
library(data.table)
library(ggplot2)
library(cowplot)
library(qqman)
library(doParallel)

#models tested
library(qgg)
library(varbvs)
library(glmnet)
library(BGLR)

#options
options(bitmapType = "cairo")
options(error = function() traceback(3))

#seed
set.seed(1)

```

The overarching goal of this process is to predict the phenotype for starvation resistance, a continuous trait, by using gene expression data, another continuous trait. This is done using k-fold cross validation to create models based on a subset of the data and calculating the correlation of that model with the remaining partition. By repeating this process multiple times with different training and testing partitions, model bias can be significantly reduced and allows for calculation of average correlation coefficients for each model. The primary difference between the methods in question is the prior distribution used.

```{r setup}

#loop count and data limit
iter <- 50

#ggplot holder list
gg <- vector(mode='list', length=2)

# result storage elements
fit_greml <- vector(mode='list', length=iter)
fit_gbayesC <- vector(mode='list', length=iter)
fit_varbvs <- vector(mode='list', length=iter)
fit_glmnet <- vector(mode='list', length=iter)
fit_bglr <- vector(mode='list', length=iter)

```



The first main set of data used for this analysis is a matrix of gene expression by DGRP line matched to raw starvation resistance. A second cluster of data sets provided by the Morgante Lab includes information on Wolbachia infection status and inversion status by line along with functions to adjust phenotypic values based on these two factors.

```{r qgg_greml, eval=FALSE, message=FALSE}

 #wolb infection and inversion status data with phenotype adjustment function
load("/data2/morgante_lab/data/dgrp/misc/adjustData.RData")

  #expression data matched to line and starvation phenotype
  xp_f <- fread("data/xp-f.txt")
  
 # xp_m <- fread("data/xp-m.txt")
  
  #setwd("C:/Users/noahk/OneDrive/Desktop/amogus")
  #getwd()
  
  #create matrix of only gene expression, trims line and starvation
  X <- as.matrix(xp_f[,3:11340])
  rownames(X) <- xp_f[,line]
  W <- scale(X)
  
  y_temp <- xp_f[,starvation]
  dat <- data.frame(id=xp_f[,line], y=y_temp)
  y <- adjustPheno(dat, "starvation")


#model to solve for, vector of ones
mu <- matrix(rep(1, length(y)), ncol=1)
#names(mu) <- paste0("line", 1:length(mu))
rownames(mu) <- xp_f[,line]
TRM <- tcrossprod(W)/ncol(W)

# k-fold parameters
n <- length(y)
fold <- 5



```


The matrix containing only gene expression by line data was then scaled to an absolute max of 1. along with this, a Translation Relationship Matrix was generated by taking the crossproduct of the scaled expression matrix and scaling it down by the number of genes.



5 was chosen for k-fold cross validation resulting in 39 lines per validation set and 159 lines per training set.


```{r trace-sample-code, eval=FALSE}
### sample analysis of gbayesC to show that convergence is working as expected 

  test_IDs <- sample(1:n, as.integer(n / fold))
  
  W_train <- W[-test_IDs,]
  W_test <- W[test_IDs,]
  y_train <- y[-test_IDs]
  y_test <- y[test_IDs]
  

  ### GBAYES-C
  fitC <- qgg::gbayes(y=y_train, W=W_train, method="bayesC", scaleY=FALSE, nit=10000, nburn=5000)
  

```

```{r trace-sample-real, eval=FALSE}


plotCustomBayes <- function(rdsPath){

# data read in is a gBayes fit object from qgg
fitC <- readRDS(rdsPath)

# calculate column narrow heritability
fitC$h2 <- fitC$vgs/(fitC$vgs+fitC$ves)

# data is extracted and stored
fitData <- data.table(iter=1:10000, ves=fitC$ves, vbs=fitC$vbs, vgs=fitC$vgs, h2=fitC$h2)

gg <- vector(mode='list', length=4)

# ggplots stored to list
gg[[1]] <- ggplot(fitData, aes(x=iter, y=ves)) +
  geom_point(size=0.5) + 
  labs(x="Iteration", y="Ve") + 
  ggtitle("Posterior Mean for Residual Variance")

gg[[2]] <- ggplot(fitData, aes(x=iter, y=vbs)) +
  geom_point(size=0.5) + 
  labs(x="Iteration", y="Vb") + 
  ggtitle("Posterior Mean for Marker Variance")

gg[[3]] <- ggplot(fitData, aes(x=iter, y=vgs)) +
  geom_point(size=0.5) + 
  labs(x="Iteration", y="Vg") + 
  ggtitle("Posterior Mean for Genomic Variance")

gg[[4]] <- ggplot(fitData, aes(x=iter, y=ves)) +
  geom_point(size=0.5) + 
  labs(x="Iteration", y="h^2") + 
  ggtitle("Posterior Mean for Narrow-sense Heritability")

  return(gg) 
}

    # fit taken from one iteration, restored from Rds using code chunk shown above
    #fitC <- readRDS("data/gbayesC-f.Rds")
    
    plotHold <- plotCustomBayes("data/gbayesC-f.Rds")

    plot_grid(plotHold[[1]],plotHold[[2]], ncol=2)
    plot_grid(plotHold[[3]],plotHold[[4]], ncol=2)
    #plotBayes(fit=fitC, what="trace")
    

```



The following methods are currently implemented:


qgg::greml - Genomic Restricted Maximum Likelihood Estimation using Best Linear Unbiased Predictor. GREML uses a Gaussian prior distribution, performing the least shrinkage and no variable selection.

glmnet - glmnet uses LASSO, or least absolute shrinkage and selection operator. Bayesian LASSO uses a thick-tailed prior which performs greater shrinkage towards the mean than a Gaussian distribution.

qgg::gbayes - BayesC has been implemented using this command. BayesC uses a spike-slab prior which performs variable selection. This intentionally sets the effect of some genes expressed to zero which models the idea that some genes have no effect on the given trait. 

varbvs - Bayesian variable selection performs variable selection using another spike-slab prior. This method avoids Markov Chain Monte Carlo methods by approximating the posterior distribution to reduce computational resources.

BGLR - Bayesian General Linear Regression has the capability to perform BayesC linear regression using a Gibbs Sampler. 


```{r loop_qgg_gbayes, message=FALSE, eval=FALSE}


#Parallel Header
#tempResult <- 

iter <- 50
corLoop <- foreach(i=1:iter) %dopar% {

#Linear Header
#for(i in 1:iter){
  
  corResult <- (1:5)
  
  #setup train and test sets with trait vectors
  test_IDs <- sample(1:n, as.integer(n / fold))
  
  W_train <- W[-test_IDs,]
  W_test <- W[test_IDs,]
  y_train <- y[-test_IDs]
  y_test <- y[test_IDs]
  

  ### GREML, qgg package
  
  fitGreml <- qgg::greml(y=y, X=mu, GRM=list(A=TRM), validate = matrix(test_IDs,ncol=1), verbose=FALSE)
  
  #Store coeff directly
  fit_greml[[i]] <- fitGreml$accuracy$Corr
  
  corResult[1] <- fitGreml$accuracy$Corr

  
  ### GBAYES-C
  
  fitC <- qgg::gbayes(y=y_train, W=W_train, method="bayesC", scaleY=FALSE, nit=10000, nburn=5000)
  
  # expected/calculated value for y_test
  # \hat{y}_test = W_{test} * \hat{b} + \hat{mu}
  y_calc <- W_test %*% fitC$b + mean(y_train)
  
  # store coeff
  fit_gbayesC[[i]] <- cor(y_test, y_calc)
  
  corResult[2] <- cor(y_test, y_calc)
  

  ### VARBVS
  fitVarb <- varbvs::varbvs(X = W_train, NULL, y=y_train, family = "gaussian", logodds=seq(-3.5,-1,0.1), sa = 1, verbose=FALSE)
  
  # \hat{y}_test = W_{test} * \hat{b} + \hat{mu}
  y_calc <- W_test %*% fitVarb$beta + mean(y_train)
  
  fit_varbvs[[i]] <- cor(y_test, y_calc)
  
  corResult[3] <- cor(y_test, y_calc)
  
  
  ### GLMNET  
  fitlm <- glmnet::cv.glmnet(x=W_train, y=y_train, alpha=1)
  
  b_hat <- glmnet::coef.glmnet(fitlm, s="lambda.min")

  y_int <- b_hat[1]

  b_hat <- b_hat[2:length(b_hat)]

  y_calc <- W_test %*% b_hat + y_int

  fit_glmnet[[i]] <- cor(y_test, y_calc)
  
  corResult[4] <- cor(y_test, y_calc)
  
  
  ### BGLR
  
  fitBG <- BGLR(y_train, response_type = "gaussian", a=NULL, b=NULL,ETA = list(list(X=W_train, model="BayesC")), nIter = 50, burnIn = 5)

y_calc <- W_test %*% fitBG$ETA[[1]]$b + mean(y_train)  

corResult[5] <- cor(y_test, y_calc)
  
  
corResult[5]

corResult

}



```


```{r listConversion-f}

iter <- 48
# results loaded from correlation loop structure
corLoop <- readRDS("code/methodComp/corLoop-f.rds")

for(i in 1:iter){
  fit_greml[[i]] <- corLoop[[i]][1]
  fit_gbayesC[[i]] <- corLoop[[i]][2]
  fit_varbvs[[i]] <- corLoop[[i]][3]
  fit_glmnet[[i]] <- corLoop[[i]][4]
  #fit_bglr[[i]] <- corLoop[[i]][5]
}

temp <- c(unlist(fit_greml), unlist(fit_gbayesC), unlist(fit_varbvs), unlist(fit_glmnet))
#, unlist(fit_bglr))

label <- c(rep("greml", iter), rep("gBayesC", iter), rep("varbvs", iter), rep("glmnet", iter))
           #, rep("bglr", iter)) 

data <- data.table(cor=as.numeric(temp), method=label)

gg[[1]] <- ggplot(data, aes(x=method, y=cor, fill=method)) +
  geom_boxplot() +
  ggtitle("Female Correlations Boxplot") +
  labs(x="Method", y="Correlation Coefficient")


```


```{r listConversion-m}

# results loaded from correlation loop structure
corLoop <- readRDS("code/methodComp/corLoop-m.rds")

for(i in 1:iter){
  fit_greml[[i]] <- corLoop[[i]][1]
  fit_gbayesC[[i]] <- corLoop[[i]][2]
  fit_varbvs[[i]] <- corLoop[[i]][3]
  fit_glmnet[[i]] <- corLoop[[i]][4]
  #fit_bglr[[i]] <- corLoop[[i]][5]
}

temp <- c(unlist(fit_greml), unlist(fit_gbayesC), unlist(fit_varbvs), unlist(fit_glmnet))
#, unlist(fit_bglr))

label <- c(rep("greml", iter), rep("gBayesC", iter), rep("varbvs", iter), rep("glmnet", iter))
#, rep("bglr", iter)) 

data <- data.table(cor=as.numeric(temp), method=label)

gg[[2]] <- ggplot(data, aes(x=method, y=cor, fill=method)) +
  geom_boxplot() +
  ggtitle("Male Correlations Boxplot") +
  labs(x="Method", y="Correlation Coefficient")


```




### Correlation Coefficient Boxplots

```{r plot-print}

plot_grid(gg[[1]],gg[[2]], ncol=2)

```





